# Text-to-SQL Training Pipeline Flow Diagram

## ğŸš€ Script Execution Flow & Logic

This document provides a comprehensive flow diagram of the `text_to_sql_train.py` script from start to finish.

---

## ğŸ“Š **MAIN EXECUTION FLOW**

```mermaid
flowchart TD
    A[ğŸ¯ Script Start] --> B[ğŸ“ Parse CLI Arguments]
    B --> C[âš™ï¸ Initialize TextToSQLConfig]
    C --> D[ğŸ”§ Apply User Configurations]
    D --> E{ğŸ® Execution Mode?}
    
    E -->|train| F[ğŸ“ TRAINING PHASE]
    E -->|deploy| G[ğŸš€ DEPLOYMENT PHASE]
    E -->|test| H[ğŸ§ª TESTING PHASE]
    E -->|full| I[ğŸ“‹ ALL PHASES]
    
    I --> F
    F --> J[ğŸ“Š Data Processing]
    J --> K[ğŸ¤– Model Training]
    K --> L[ğŸ“ˆ Model Evaluation]
    L --> M{Mode = full?}
    M -->|Yes| G
    M -->|No| END[âœ… Complete]
    
    G --> N[ğŸ”— Ollama Deployment]
    N --> O{Mode = full?}
    O -->|Yes| H
    O -->|No| END
    
    H --> P[ğŸ§ª Model Testing]
    P --> END
    
    style A fill:#e1f5fe
    style END fill:#c8e6c9
    style F fill:#fff3e0
    style G fill:#f3e5f5
    style H fill:#e8f5e8
```

---

## ğŸ”§ **CONFIGURATION & INITIALIZATION PHASE**

```mermaid
flowchart TD
    A[ğŸ“ Parse Arguments] --> B{Device Specified?}
    B -->|auto| C[ğŸ” Auto-detect Device]
    B -->|forced| D[ğŸ¯ Use Forced Device]
    
    C --> E{Device Type?}
    E -->|Mac MPS| F[ğŸ Apply Mac Optimizations]
    E -->|NVIDIA GPU| G[ğŸŸ¢ Apply NVIDIA Optimizations]
    E -->|CPU| H[ğŸ’» Apply CPU Optimizations]
    
    D --> I{Forced Device Type?}
    I -->|mps| F
    I -->|cuda| G
    I -->|cpu| H
    
    F --> J[âš¡ Small batch, fast settings]
    G --> K[ğŸš„ Large batch, high performance]
    H --> L[ğŸŒ Conservative settings]
    
    J --> M[ğŸ“‹ Configuration Complete]
    K --> M
    L --> M
    
    style A fill:#e3f2fd
    style M fill:#c8e6c9
    style F fill:#fff8e1
    style G fill:#e8f5e8
    style H fill:#fce4ec
```

---

## ğŸ“Š **DATA PROCESSING PHASE**

```mermaid
flowchart TD
    A[ğŸ Start Data Processing] --> B[ğŸ“¥ Load Gretel AI Dataset]
    B --> C{Max Samples Set?}
    C -->|Yes| D[âœ‚ï¸ Limit Dataset Size]
    C -->|No| E[ğŸ“‹ Use Full Dataset]
    
    D --> F[ğŸ”„ Format Samples for Training]
    E --> F
    
    F --> G[ğŸ“ Create Instruction-Tuning Format]
    G --> H{Stratification Possible?}
    
    H -->|Yes| I[ğŸ¯ Stratified Train/Val Split]
    H -->|No| J[ğŸ² Random Train/Val Split]
    
    I --> K[ğŸ’¾ Save Processed Data]
    J --> K
    K --> L[âœ… Data Processing Complete]
    
    style A fill:#e8f5e8
    style L fill:#c8e6c9
    style G fill:#fff3e0
```

### **Data Format Structure:**
```
Input Format:
- Instruction: "You are an expert SQL generator for [domain]..."
- Input: "Database Schema: [schema]\nRequest: [question]"
- Output: "```sql\n[query]\n```\nExplanation: [explanation]"
```

---

## ğŸ¤– **MODEL TRAINING PHASE**

```mermaid
flowchart TD
    A[ğŸ Start Model Training] --> B[ğŸ” Detect Device Capabilities]
    B --> C[ğŸ“¦ Load Base Model & Tokenizer]
    C --> D{Quantization Enabled?}
    
    D -->|Yes| E[âš¡ Setup 4-bit Quantization]
    D -->|No| F[ğŸ’½ Load Full Precision]
    
    E --> G{Load Successful?}
    F --> G
    G -->|No| H[ğŸ”„ Fallback Strategy]
    G -->|Yes| I[ğŸ›ï¸ Setup LoRA Configuration]
    
    H --> I
    I --> J[ğŸ”§ Configure Training Arguments]
    J --> K[ğŸƒ Execute Training Loop]
    K --> L[ğŸ’¾ Save Model/Adapter]
    L --> M[âœ… Training Complete]
    
    style A fill:#e8f5e8
    style M fill:#c8e6c9
    style K fill:#fff3e0
```

### **LoRA Configuration:**
- **Target Modules**: `["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]`
- **Rank (r)**: `16` (adaptation matrix size)
- **Alpha**: `32` (scaling factor)
- **Dropout**: `0.1` (regularization)

---

## ğŸš€ **DEPLOYMENT PHASE**

```mermaid
flowchart TD
    A[ğŸ Start Deployment] --> B[ğŸ—ºï¸ Map HF Model to Ollama]
    B --> C{Base Model Available?}
    C -->|No| D[ğŸ“¥ Pull Base Model]
    C -->|Yes| E[ğŸ“ Create Modelfile]
    
    D --> F{Pull Successful?}
    F -->|No| G[ğŸ”„ Try Alternative Model]
    F -->|Yes| E
    G --> E
    
    E --> H[ğŸ—ï¸ Build Ollama Model]
    H --> I{Build Successful?}
    I -->|Yes| J[âœ… Deployment Complete]
    I -->|No| K[âŒ Deployment Failed]
    
    style A fill:#f3e5f5
    style J fill:#c8e6c9
    style K fill:#ffcdd2
```

### **Model Mapping Examples:**
- `codellama/CodeLlama-7b-Instruct-hf` â†’ `codellama:7b`
- `defog/sqlcoder-7b` â†’ `codellama:7b` (fallback)

---

## ğŸ§ª **TESTING PHASE**

```mermaid
flowchart TD
    A[ğŸ Start Testing] --> B[ğŸ“‹ Create Test Cases]
    B --> C[ğŸ”„ For Each Test Case]
    C --> D[ğŸ“¤ Send Query to Ollama]
    D --> E[ğŸ“¥ Receive SQL Response]
    E --> F[ğŸ“Š Log Results]
    F --> G{More Tests?}
    G -->|Yes| C
    G -->|No| H[âœ… Testing Complete]
    
    style A fill:#e8f5e8
    style H fill:#c8e6c9
    style D fill:#fff3e0
```

---

## ğŸ“± **COMMAND-LINE INTERFACE FLOW**

```mermaid
flowchart TD
    A[âŒ¨ï¸ CLI Input] --> B{Mode?}
    
    B -->|--mode train| C[ğŸ“ Training Only]
    B -->|--mode deploy| D[ğŸš€ Deployment Only]
    B -->|--mode test| E[ğŸ§ª Testing Only]
    B -->|--mode full| F[ğŸ“‹ Complete Pipeline]
    
    G[âš™ï¸ Options Applied]
    
    C --> G
    D --> G
    E --> G
    F --> G
    
    H[ğŸ”§ Device Options]
    I[ğŸ“Š Data Options]
    J[ğŸ¤– Model Options]
    
    H --> G
    I --> G
    J --> G
    
    style A fill:#e3f2fd
    style G fill:#c8e6c9
```

### **CLI Options Categories:**

#### **ğŸ”§ Device & Performance:**
- `--device [auto|cpu|cuda|mps]` - Force specific device
- `--fast-mac` - Aggressive Mac optimizations
- `--cpu-mode` - CPU-only with optimizations
- `--no-quantization` - Disable 4-bit quantization

#### **ğŸ“Š Data & Training:**
- `--max-samples N` - Limit training samples
- `--save-full-model` - Save complete model (not just LoRA)

#### **ğŸ¤– Model Configuration:**
- `--base-model MODEL` - Override default base model
- `--model-name NAME` - Custom Ollama model name

---

## ğŸ› ï¸ **ERROR HANDLING & FALLBACK STRATEGIES**

```mermaid
flowchart TD
    A[âŒ Error Detected] --> B{Error Type?}
    
    B -->|Model Loading| C[ğŸ”„ Quantization Fallback]
    B -->|Device| D[ğŸ”„ Device Fallback]
    B -->|Dataset| E[ğŸ”„ Data Fallback]
    B -->|Deployment| F[ğŸ”„ Ollama Fallback]
    
    C --> G[ğŸ’½ Load without quantization]
    D --> H[ğŸ’» Fall back to CPU]
    E --> I[ğŸ² Simple random split]
    F --> J[ğŸ“¥ Try alternative base model]
    
    G --> K{Success?}
    H --> K
    I --> K
    J --> K
    
    K -->|Yes| L[âœ… Continue Execution]
    K -->|No| M[âŒ Fatal Error]
    
    style A fill:#ffcdd2
    style L fill:#c8e6c9
    style M fill:#f44336
```

---

## ğŸ“‹ **MEMORY OPTIMIZATION STRATEGIES**

### **ğŸ¯ Device-Specific Optimizations:**

| Device | Batch Size | Quantization | Precision | Special Settings |
|--------|------------|--------------|-----------|------------------|
| **ğŸ Mac MPS** | 2 | Disabled | fp32 | Gradient checkpointing off |
| **ğŸŸ¢ NVIDIA GPU** | 8 | 4-bit NF4 | bf16/fp16 | Flash attention |
| **ğŸ’» CPU** | 1 | Disabled | fp32 | Conservative settings |

### **ğŸ§  Memory Management:**
- **Gradient Checkpointing**: Trade compute for memory
- **LoRA Adapters**: Only train small adapter layers
- **4-bit Quantization**: 75% memory reduction
- **Batch Accumulation**: Simulate larger batches

---

## ğŸ¯ **EXECUTION EXAMPLES**

### **ğŸš€ Quick Mac Development:**
```bash
python text_to_sql_train.py --fast-mac --max-samples 500
```

### **ğŸ’ª Full GPU Training:**
```bash
python text_to_sql_train.py --mode full --device cuda
```

### **ğŸ–¥ï¸ CPU-Only Training:**
```bash
python text_to_sql_train.py --cpu-mode --max-samples 1000
```

### **ğŸ“¦ Deploy Only:**
```bash
python text_to_sql_train.py --mode deploy --model-name my-sql-model
```

---

## ğŸ“ˆ **Performance Monitoring**

The script includes comprehensive logging at every stage:

- **ğŸ“Š Data Processing**: Sample counts, formatting success rate
- **ğŸ¤– Model Training**: Loss curves, device utilization, memory usage
- **ğŸš€ Deployment**: Model creation status, Ollama integration
- **ğŸ§ª Testing**: Query success rate, response quality

All logs are saved to `text_to_sql_training.log` for debugging and analysis.

---

## ğŸ‰ **Success Criteria**

1. **âœ… Training**: Model loss decreases, adapters saved successfully
2. **âœ… Deployment**: Ollama model created and accessible
3. **âœ… Testing**: Model generates syntactically correct SQL
4. **âœ… Integration**: End-to-end pipeline completes without errors

This flow ensures a robust, device-agnostic text-to-SQL model training and deployment pipeline!
